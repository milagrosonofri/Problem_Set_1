{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milagrosonofri/Problem_Set_1/blob/main/PS1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtención y limpieza de la base de datos"
      ],
      "metadata": {
        "id": "H8j9L3o5AxL4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wqvuo0LbINWz"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías necesarias\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import LeaveOneOut"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conexión con GitHub\n",
        "!git clone https://github.com/milagrosonofri/Problem_Set_1.git\n",
        "%cd Problem_Set_1/views"
      ],
      "metadata": {
        "id": "2t3n7Old2GXX",
        "outputId": "0aac24d2-cbbf-4147-8ccd-614fef548035",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Problem_Set_1' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: 'Problem_Set_1/views'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Scraping"
      ],
      "metadata": {
        "id": "PTcFCde5IdHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Armamos una lista para almacenar todos los datos\n",
        "data = []\n",
        "\n",
        "# Base URL de las páginas\n",
        "base_url = 'https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_{}.html'\n",
        "\n",
        "# Iteramos sobre las 10 páginas\n",
        "for i in range(1, 11):\n",
        "\n",
        "    # Construimos la URL de la página actual\n",
        "    url = base_url.format(i)\n",
        "\n",
        "    # Enviamos solicitud HTTP\n",
        "    response = requests.get(url)\n",
        "    print(f\"Scraping página {i}: {response}\")  # Confirmar que la solicitud es exitosa (<Response [200]>)\n",
        "\n",
        "    # Parsear el contenido HTML\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Encontrar la tabla en la página\n",
        "    table = soup.find('table')\n",
        "\n",
        "    # Extraemos encabezados (solo en la primera iteración)\n",
        "    if i == 1:  # Tomar encabezados solo de la primera página\n",
        "        headers = []\n",
        "        for header in table.find_all('th'):\n",
        "            headers.append(header.text.strip())\n",
        "\n",
        "    # Extraemos filas de datos\n",
        "    for row in table.find_all('tr'):\n",
        "        row_data = []\n",
        "        for cell in row.find_all('td'):\n",
        "            row_data.append(cell.text.strip())\n",
        "        if row_data:  # Agregar solo filas con datos\n",
        "            data.append(row_data)\n",
        "\n",
        "# Convertimos los datos consolidados en un DataFrame\n",
        "df = pd.DataFrame(data, columns=headers)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "j3pywVoBIgZO",
        "outputId": "f29d6611-ddfd-4826-cb3a-6053f93ea233"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping página 1: <Response [200]>\n",
            "Scraping página 2: <Response [200]>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-47583104a132>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Parsear el contenido HTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Encontrar la tabla en la página\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/html/parser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/html/parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstarttagopen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# < + letter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/html/parser.py\u001b[0m in \u001b[0;36mparse_starttag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_startendtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDATA_CONTENT_ELEMENTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cdata_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mhandle_starttag\u001b[0;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m#print(\"START\", name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0msourceline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msourcepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         tag = self.soup.handle_starttag(\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msourceline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msourceline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0msourcepos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msourcepos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mhandle_starttag\u001b[0;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         tag = self.element_classes.get(Tag, Tag)(\n\u001b[0m\u001b[1;32m    750\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentTag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_most_recent_element\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/element.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parser, builder, name, namespace, prefix, attrs, parent, previous, is_xml, sourceline, sourcepos, can_be_empty_element, cdata_list_attributes, preserve_whitespace_tags, interesting_string_types, namespaces)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/element.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, parent, previous_element, next_element, previous_sibling, next_sibling)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mknown_xml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     def setup(self, parent=None, previous_element=None, next_element=None,\n\u001b[0m\u001b[1;32m    157\u001b[0m               previous_sibling=None, next_sibling=None):\n\u001b[1;32m    158\u001b[0m         \"\"\"Sets up the initial relations between this element and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpieza de base de datos"
      ],
      "metadata": {
        "id": "HxBl-X0rA4D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtengo el nombre de todas las variables disponibles\n",
        "variables = df.columns.tolist()\n",
        "\n",
        "print(variables)\n",
        "\n",
        "# Restringimos a individuos empleados mayores de 18 años (HAY QUE REVISAR SI P6240 ES LA CORRECTA)\n",
        "df_limpio = df[(df['age'].astype(float) > 18) & (df['ocu'] == '1')]\n",
        "\n",
        "print(f\"Número de registros después del filtro (empleados mayores de 18 años): {len(df_limpio)}\")"
      ],
      "metadata": {
        "id": "wd-S9dexy9oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Primer approach para describir los datos\n",
        "\n",
        "print(\"Información del DataFrame:\")\n",
        "print(df_limpio.info())"
      ],
      "metadata": {
        "id": "JoQrXE6Ly9k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renombro variables para tener una interpretación mas sencilla de aquí en adelante\n",
        "df_limpio = df_limpio.rename(columns={'p6500': 'salario_empleo_principal', 'p7070': 'salario_empleo_secundario',\n",
        "                                      'p6426': 'antig', 'sex': 'sexo', 'age': 'edad'})"
      ],
      "metadata": {
        "id": "RE3AO9i1NiyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convierto algunas variables de relacionados al salario horario a formato numérico para poder compararlas\n",
        "wage_hours_related_vars = ['salario_empleo_principal', 'salario_empleo_secundario', 'hoursWorkUsual', 'hoursWorkActualSecondJob',\n",
        "                           'y_salary_m', 'y_salary_m_hu', 'y_ingLab_m', 'y_ingLab_m_ha']\n",
        "\n",
        "for col in wage_hours_related_vars:\n",
        "\n",
        "    df_limpio[col] = pd.to_numeric(df_limpio[col], errors='coerce')\n",
        "    print(f\"{col} dtype: {df_limpio[col].dtype}\")\n",
        "\n",
        "# Calculo 'hour_wage' teniendo en cuenta los valores NaN\n",
        "df_limpio['hour_wage'] = (df_limpio['salario_empleo_principal'].fillna(0) + df_limpio['salario_empleo_secundario'].fillna(0)) / \\\n",
        "                                         ((df_limpio['hoursWorkActualSecondJob'].fillna(0) + df_limpio['hoursWorkUsual'].fillna(0))*4)\n",
        "\n",
        "# Ajusto para casos donde el denominador es cero (para evitar divisiones por cero)\n",
        "df_limpio.loc[(df_limpio['hoursWorkActualSecondJob'].fillna(0) + df_limpio['hoursWorkUsual'].fillna(0)) == 0, 'hour_wage'] = np.nan\n",
        "\n",
        "# Ajusto para casos donde ambos, salario_empleo_principal y salario_empleo_secundario, son NaN\n",
        "df_limpio.loc[df_limpio['salario_empleo_principal'].isna() & df_limpio['salario_empleo_secundario'].isna(), 'hour_wage'] = np.nan\n",
        "\n",
        "# Ajusto para casos donde ambos, hoursWorkActualSecondJob y hoursWorkUsual, son NaN\n",
        "df_limpio.loc[df_limpio['hoursWorkActualSecondJob'].isna() & df_limpio['hoursWorkUsual'].isna(), 'hour_wage'] = np.nan\n",
        "\n",
        "# Agrego la variable creada a la lista con las variables a comparar\n",
        "wage_hours_related_vars = wage_hours_related_vars + ['hour_wage']"
      ],
      "metadata": {
        "id": "FA_dWNtE36iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observo variables de interés\n",
        "print(\"Unique values in 'maxEducLevel':\")\n",
        "print(df_limpio['maxEducLevel'].unique())\n",
        "\n",
        "print(\"\\nUnique values in 'sizeFirm':\")\n",
        "print(df_limpio['sizeFirm'].unique())\n",
        "\n",
        "print(\"\\nUnique values in 'relab':\")\n",
        "print(df_limpio['relab'].unique())\n",
        "\n",
        "print(\"\\nUnique values in 'informal':\")\n",
        "print(df_limpio['informal'].unique())\n",
        "\n",
        "print(\"\\nUnique values in 'sex':\")\n",
        "print(df_limpio['sexo'].unique())\n",
        "\n",
        "print(\"\\nUnique values in 'oficio':\")\n",
        "print(df_limpio['oficio'].unique())"
      ],
      "metadata": {
        "id": "YoJgSQjR_pAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convierto a las que van a ser mis variables independientes a tipo numérico, manejando posibles valores no numéricos (como 'NA')\n",
        "\n",
        "df_limpio['maxEducLevel'] = pd.to_numeric(df_limpio['maxEducLevel'], errors='coerce')\n",
        "df_limpio['antig'] = pd.to_numeric(df_limpio['antig'], errors='coerce')\n",
        "df_limpio['edad'] = pd.to_numeric(df_limpio['edad'], errors='coerce')\n",
        "df_limpio['sizeFirm'] = pd.to_numeric(df_limpio['sizeFirm'], errors='coerce')\n",
        "df_limpio['relab'] = pd.to_numeric(df_limpio['relab'], errors='coerce')\n",
        "df_limpio['informal'] = pd.to_numeric(df_limpio['informal'], errors='coerce')\n",
        "df_limpio['sexo'] = pd.to_numeric(df_limpio['sexo'], errors='coerce')\n",
        "df_limpio['oficio'] = pd.to_numeric(df_limpio['oficio'], errors='coerce')"
      ],
      "metadata": {
        "id": "tm4XSstp8TLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diccionario de agrupaciones\n",
        "agrupaciones = {\n",
        "    \"Ciencias, Ingeniería y Tecnología\": [1, 2, 3, 4, 8],\n",
        "    \"Ciencias Biológicas y de la Salud\": [5, 6, 7],\n",
        "    \"Ciencias Sociales y Humanidades\": [9, 11, 12, 13, 19],\n",
        "    \"Arte, Cultura y Medios\": [15, 16, 17, 18, 92],\n",
        "    \"Administración y Gestión\": [20, 21, 30],\n",
        "    \"Oficios Administrativos y Comerciales\": [31, 32, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 49],\n",
        "    \"Servicios Personales y Comunitarios\": [14, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
        "    \"Agricultura, Pesca y Explotación de Recursos Naturales\": [60, 61, 62, 63, 64, 71, 72, 73],\n",
        "    \"Industria, Manufactura y Construcción\": [70, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96],\n",
        "    \"Transporte y Logística\": [35, 36, 37, 97, 98],\n",
        "    \"Otros Oficios y Trabajos No Clasificados\": [99]\n",
        "}\n",
        "\n",
        "# Función para asignar la nueva categoría\n",
        "def asignar_categoria(codigo):\n",
        "    for categoria, codigos in agrupaciones.items():\n",
        "        if codigo in codigos:\n",
        "            return categoria\n",
        "    return \"Sin Categoría\"  # En caso de que algún código no esté clasificado\n",
        "\n",
        "# Aplicar la función a cada fila en la columna \"oficio\" para crear la nueva columna \"oficio_new\"\n",
        "df_limpio['oficio_new'] = df_limpio['oficio'].apply(asignar_categoria)"
      ],
      "metadata": {
        "id": "f0hp_DNhzebY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_limpio['oficio_new'].unique())"
      ],
      "metadata": {
        "id": "PYD05YFgznzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genero dummies para variables categóricas\n",
        "categorical_vars = [\"relab\", \"sizeFirm\", \"maxEducLevel\", \"oficio_new\"]\n",
        "df_encoded = pd.get_dummies(df_limpio, columns=categorical_vars, drop_first = True)\n",
        "\n",
        "# Defino un subset con las variables de interés con el format correcto\n",
        "variables = [\"y_ingLab_m_ha\", \"informal\", \"antig\", \"sexo\", \"edad\"] + \\\n",
        "            [col for col in df_encoded.columns if any(var in col for var in categorical_vars)]\n",
        "\n",
        "subset = df_encoded[variables]"
      ],
      "metadata": {
        "id": "rEcfRKUq_JC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_limpio)"
      ],
      "metadata": {
        "id": "I_HW4rCY34Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descripción de los datos"
      ],
      "metadata": {
        "id": "1xsQ3msPFvfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estadisticos descriptivos para asociadas al salario horario\n",
        "\n",
        "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
        "\n",
        "for col in wage_hours_related_vars:\n",
        "\n",
        "  print(\"\\nEstadísticas descriptivas:\")\n",
        "  print(df_limpio[col].describe())\n",
        "\n",
        "  zero_count = (df_limpio[col] == 0).sum()\n",
        "  na_count = df_limpio[col].isna().sum()\n",
        "  print(f\"Number of zeros in '{col}': {zero_count}\")\n",
        "  print(f\"Number of missing values (NA) in '{col}': {na_count}\")"
      ],
      "metadata": {
        "id": "NYQb4QCS30vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diccionario con nombres descriptivos para las variables\n",
        "etiquetas = {\n",
        "    \"salario_empleo_principal\": \"Salario del empleo principal\",\n",
        "    \"salario_empleo_secundario\": \"Salario del empleo secundario\",\n",
        "    \"hoursWorkUsual\": \"Horas trabajadas\",\n",
        "    \"hoursWorkActualSecondJob\": \"Horas trabajadas en el segundo empleo\",\n",
        "    \"y_salary_m\": \"Salario nominal mensual en empleo principal\",\n",
        "    \"y_salary_m_hu\": \"Salario real horario en empleo principal\",\n",
        "    \"y_ingLab_m\": \"Ingreso laboral nominal mensual\",\n",
        "    \"y_ingLab_m_ha\": \"Ingreso laboral nominal horario\",\n",
        "    \"hour_wage\": \"Salario horario\"\n",
        "}\n",
        "\n",
        "\n",
        "# Graficar histogramas\n",
        "for col in wage_hours_related_vars:\n",
        "    data = df_limpio[col].dropna()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))  # Tamaño de la figura\n",
        "    plt.hist(data, bins=30, edgecolor='black', alpha=0.7)\n",
        "    plt.title(f\"Histograma de {etiquetas.get(col, col)}\", fontsize=14)  # Título claro\n",
        "    plt.xlabel(etiquetas.get(col, col), fontsize=12)\n",
        "    plt.ylabel(\"Frecuencia\", fontsize=12)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.tight_layout()  # Ajusta los márgenes automáticamente\n",
        "    plt.show()\n",
        "\n",
        "    # Guardo el plot como PNG file\n",
        "    plt.savefig(f\"./{col}_histograma.png\", dpi=300)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "YFCbf0qry9d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficamos histogramas del logaritmo natural\n",
        "for col in wage_hours_related_vars:\n",
        "    data = df_limpio[col].dropna()\n",
        "\n",
        "    # Excluir valores no positivos para calcular logaritmo natural\n",
        "    data_log = data[data > 0].apply(np.log)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))  # Tamaño de la figura\n",
        "    plt.hist(data_log, bins=30, color='b', edgecolor='black', alpha=0.7)  # Color azul con bordes negros\n",
        "    plt.title(f\"Logaritmo natural de {etiquetas.get(col, col)}\", fontsize=14)  # Título claro\n",
        "    plt.xlabel(f\"{etiquetas.get(col, col)} (en logaritmo)\", fontsize=12)  # Etiqueta del eje x\n",
        "    plt.ylabel(\"Frecuencia\", fontsize=12)  # Etiqueta del eje y\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.tight_layout()  # Ajusta los márgenes automáticamente\n",
        "    plt.show()\n",
        "\n",
        "    # Guardo el plot como PNG file\n",
        "    plt.savefig(f\"{col}_log_histograma.png\", dpi=300)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "mu5CT_-rWXL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculo y visualizo la matriz de correlación\n",
        "corr_matrix = subset.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, cmap=\"coolwarm\", fmt=\".2f\", cbar=True)\n",
        "plt.title(\"Matriz de correlación\")\n",
        "plt.show()\n",
        "\n",
        "# Guardo el plot como PNG file\n",
        "plt.savefig(f\"./matriz_correlacion.png\", dpi=300)\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "0MidKgN6FySy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elimino missings\n",
        "\n",
        "df_limpio = df_limpio[[\"y_ingLab_m_ha\", \"edad\", \"informal\", \"antig\", \"relab\", \"sexo\", \"sizeFirm\",\"maxEducLevel\", \"oficio_new\"]]\n",
        "df_limpio = df_limpio.dropna()"
      ],
      "metadata": {
        "id": "vSXmc7lJ6Wdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino las variables independientes y la dependiente\n",
        "variables_independientes = [\"edad\", \"informal\", \"relab\", \"antig\", \"sexo\", \"sizeFirm\",\"maxEducLevel\", \"oficio_new\"]\n",
        "\n",
        "variable_dependiente = \"y_ingLab_m_ha\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Diccionario para renombrar variables con nombres más descriptivos\n",
        "etiquetas = {\n",
        "    \"edad\": \"Edad\",\n",
        "    \"informal\": \"Trabajo informal\",\n",
        "    \"relab\": \"Relación laboral\",\n",
        "    \"antig\": \"Antigüedad\",\n",
        "    \"sexo\": \"Sexo\",\n",
        "    \"sizeFirm\": \"Tamaño de la firma\",\n",
        "    \"maxEducLevel\": \"Máximo nivel educativo\",\n",
        "    \"oficio_new\": \"Oficio\",\n",
        "    \"y_ingLab_m_ha\": \"Salario horario\"\n",
        "}\n",
        "\n",
        "# Creo scatterplots entre la variable dependiente y cada variable independiente\n",
        "for var in variables_independientes:\n",
        "    plt.figure(figsize=(8, 6))  # Tamaño de la figura\n",
        "    plt.scatter(df_limpio[var], df_limpio[variable_dependiente], alpha=0.7)\n",
        "    plt.title(f\"{etiquetas.get(var, var)} vs {etiquetas[variable_dependiente]}\", fontsize=14)\n",
        "    plt.xlabel(etiquetas.get(var, var), fontsize=12)\n",
        "    plt.ylabel(etiquetas[variable_dependiente], fontsize=12)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.tight_layout()  # Ajusta el diseño para evitar solapamientos\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Guardo el plot como PNG file\n",
        "    plt.savefig(f\"./{var}_sal_horario_scatterplot.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "-YaFkWAMOwmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos"
      ],
      "metadata": {
        "id": "oiUvSLRaEcn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset = subset.dropna()"
      ],
      "metadata": {
        "id": "gxBPd1aQ8OC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos variables dependiente e independientes, el train set y el test set\n",
        "\n",
        "target = \"y_ingLab_m_ha\"\n",
        "\n",
        "X = subset.drop(columns=[target])\n",
        "y = np.log(subset[target])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                        X,\n",
        "                                        y,\n",
        "                                        test_size=0.3,\n",
        "                                        train_size=0.7,\n",
        "                                        random_state = 123\n",
        "                                        )\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "25_Piqa1EgCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the list of new columns\n",
        "print(X_train.columns)"
      ],
      "metadata": {
        "id": "7KdTm6vF7CnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_related_columns(df, categorical_vars):\n",
        "    \"\"\"\n",
        "    Get all columns in a DataFrame related to specific categorical variables.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the dummy variables.\n",
        "        categorical_vars (list): A list of prefixes for the categorical variables.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are the variable prefixes and values are lists of column names.\n",
        "    \"\"\"\n",
        "    related_columns = {}\n",
        "    for var in categorical_vars:\n",
        "        # Encuentro a todas las variables relacionadas con la variable categórica\n",
        "        related_columns[var] = [col for col in df.columns if var in col]\n",
        "    return related_columns\n",
        "\n",
        "# Lista de variables categóricas\n",
        "categorical_vars = [\"relab\", \"sizeFirm\", \"maxEducLevel\", \"oficio_new\"]\n",
        "\n",
        "related_columns_dict = get_related_columns(X_train, categorical_vars)\n",
        "\n",
        "for var, columns in related_columns_dict.items():\n",
        "    print(f\"{var}: {columns}\")\n"
      ],
      "metadata": {
        "id": "pJVk9Y3u913-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 1"
      ],
      "metadata": {
        "id": "bCsWa2F7Er6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el modelo\n",
        "X0 = np.ones((len(y_train), 1))\n",
        "model1= LinearRegression().fit(X0,y_train)\n",
        "model1.intercept_"
      ],
      "metadata": {
        "id": "dHWvEM_-EkSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Media estimada\n",
        "y_train.mean()"
      ],
      "metadata": {
        "id": "iVihOaTgEv0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 1\n",
        "X0_test = np.ones((len(y_test), 1))\n",
        "y_hat_model1 = model1.predict(X0_test)\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse1 = mean_squared_error(y_test, y_hat_model1)\n",
        "rmse1 = np.sqrt(mse1)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse1}')"
      ],
      "metadata": {
        "id": "wVvDCyMGFS2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 2: Intercepto y máximo nivel educativo alcanzado"
      ],
      "metadata": {
        "id": "mWeY4AemFdiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agarramos las dummies relevantes, en este caso las de nivel educativo\n",
        "columns_to_use_2 = related_columns_dict[\"maxEducLevel\"]"
      ],
      "metadata": {
        "id": "K1ZI6HW59-PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el modelo\n",
        "model2=  LinearRegression().fit(X_train[columns_to_use_2],y_train)\n",
        "model2.coef_"
      ],
      "metadata": {
        "id": "gk33WRESFcsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 2\n",
        "y_hat_model2 = model2.predict(X_test[columns_to_use_2])\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse2 = mean_squared_error(y_test, y_hat_model2)\n",
        "rmse2 = np.sqrt(mse2)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse2}')"
      ],
      "metadata": {
        "id": "XahWIzMhFjFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 3: Intercepto, máximo nivel educativo alcanzado, edad y sexo"
      ],
      "metadata": {
        "id": "8gGYXoL2FoIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el modelo\n",
        "model3=  LinearRegression().fit(X_train[[\"edad\", \"informal\",\"sexo\"]+columns_to_use_2],y_train)\n",
        "model3.coef_"
      ],
      "metadata": {
        "id": "csqYc6E5FnZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 3\n",
        "y_hat_model3 = model3.predict(X_test[[\"edad\", \"informal\",\"sexo\"]+columns_to_use_2])\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse3 = mean_squared_error(y_test, y_hat_model3)\n",
        "rmse3 = np.sqrt(mse3)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse3}')"
      ],
      "metadata": {
        "id": "wbhwtolWFyHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 4: Intercepto, máximo nivel educativo alcanzado, edad, sexo, formalidad del empleo y tipo de relación laboral"
      ],
      "metadata": {
        "id": "uBt58aVeC_v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agarramos las dummies relevantes, en este caso las de nivel educativo y relación laboral\n",
        "columns_to_use_4 = related_columns_dict[\"relab\"] + related_columns_dict[\"maxEducLevel\"]"
      ],
      "metadata": {
        "id": "zkXg0B2DAPRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "model4=  LinearRegression().fit(X_train[[\"edad\", \"informal\", \"sexo\"]+columns_to_use_4],y_train)\n",
        "model4.coef_"
      ],
      "metadata": {
        "id": "cCQ7oaMOC-ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 4\n",
        "y_hat_model4 = model4.predict(X_test[[\"edad\", \"informal\", \"sexo\"]+columns_to_use_4])\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse4 = mean_squared_error(y_test, y_hat_model4)\n",
        "rmse4 = np.sqrt(mse4)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse4}')"
      ],
      "metadata": {
        "id": "bTwuw2flDMu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 5: Intercepto, máximo nivel educativo alcanzado, edad, sexo, formalidad del empleo, tipo de relación laboral, antiguedad y tamaño de la firma"
      ],
      "metadata": {
        "id": "_7SlrhpFSnKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agarramos las dummies relevantes, en este caso las de nivel educativo y relación laboral\n",
        "columns_to_use_5 = related_columns_dict[\"relab\"] + related_columns_dict[\"maxEducLevel\"] + related_columns_dict[\"sizeFirm\"]"
      ],
      "metadata": {
        "id": "bb9nU_fhA-a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "model5=  LinearRegression().fit(X_train[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_5],y_train)\n",
        "model5.coef_"
      ],
      "metadata": {
        "id": "OXuwRSmjSoSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 5\n",
        "y_hat_model5 = model5.predict(X_test[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_5])\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse5 = mean_squared_error(y_test, y_hat_model5)\n",
        "rmse5 = np.sqrt(mse5)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse5}')"
      ],
      "metadata": {
        "id": "C1h5-BdQSqP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 6: Intercepto, máximo nivel educativo alcanzado, edad, sexo, formalidad del empleo, tipo de relación laboral, antiguedad, tamaño de la firma y oficio\n",
        "\n"
      ],
      "metadata": {
        "id": "-afCIq-4TaVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agarramos las dummies relevantes, en este caso las de nivel educativo, relación laboral y oficio\n",
        "columns_to_use_6 = related_columns_dict[\"relab\"] + related_columns_dict[\"maxEducLevel\"] + related_columns_dict[\"sizeFirm\"] + related_columns_dict[\"oficio_new\"]"
      ],
      "metadata": {
        "id": "hEo8KI8NCowc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "model6=  LinearRegression().fit(X_train[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_6],y_train)\n",
        "model6.coef_"
      ],
      "metadata": {
        "id": "IHyv9YuPCvb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 6\n",
        "y_hat_model6 = model6.predict(X_test[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_6])\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse6 = mean_squared_error(y_test, y_hat_model6)\n",
        "rmse6 = np.sqrt(mse6)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse6}')"
      ],
      "metadata": {
        "id": "-ves2m5UCyVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 7: Modelo 3 con interacciones y no linealidades de segundo grado"
      ],
      "metadata": {
        "id": "nT42jRctTJ45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_2 = PolynomialFeatures(degree=2)"
      ],
      "metadata": {
        "id": "WngSOW565MIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly_3 = PolynomialFeatures(degree=3)"
      ],
      "metadata": {
        "id": "E0ZbfpmV5Nm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "X_train_poly_2 = poly_2.fit_transform(X_train[[\"edad\",\"informal\",\"sexo\"]+columns_to_use_2])\n",
        "model7 =  LinearRegression().fit(X_train_poly_2,y_train)"
      ],
      "metadata": {
        "id": "BDCVT2KxTKwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 7\n",
        "X_test_poly_7 = poly_2.fit_transform(X_test[[\"edad\", \"informal\",\"sexo\"]+columns_to_use_2])\n",
        "y_hat_model7 = model7.predict(X_test_poly_7)\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse7 = mean_squared_error(y_test, y_hat_model7)\n",
        "rmse7 = np.sqrt(mse7)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse7}')"
      ],
      "metadata": {
        "id": "uY2BhI8sTMrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 8: Modelo 3 con interacciones y no linealidades de tercer grado"
      ],
      "metadata": {
        "id": "KGpErTp2TOLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "X_train_poly_3 = poly_3.fit_transform(X_train[[\"edad\",\"informal\",\"sexo\"]+columns_to_use_2])\n",
        "model8 =  LinearRegression().fit(X_train_poly_3,y_train)"
      ],
      "metadata": {
        "id": "aIaN0B8nTUeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 8\n",
        "X_test_poly_8 = poly_3.fit_transform(X_test[[\"edad\", \"informal\",\"sexo\"]+columns_to_use_2])\n",
        "y_hat_model8 = model8.predict(X_test_poly_8)\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse8 = mean_squared_error(y_test, y_hat_model8)\n",
        "rmse8 = np.sqrt(mse8)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse8}')"
      ],
      "metadata": {
        "id": "8nQNf3d9TV6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 9: Modelo 4 con interacciones y no linealidades de segundo grado"
      ],
      "metadata": {
        "id": "tlojpdD_MI5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "X_train_poly_2 = poly_2.fit_transform(X_train[[\"edad\", \"informal\", \"sexo\"]+columns_to_use_4])\n",
        "model9 =  LinearRegression().fit(X_train_poly_2,y_train)"
      ],
      "metadata": {
        "id": "HHlwN-LLMORE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 9\n",
        "X_test_poly_9 = poly_2.fit_transform(X_test[[\"edad\", \"informal\", \"sexo\"]+columns_to_use_4])\n",
        "y_hat_model9 = model9.predict(X_test_poly_9)\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse9 = mean_squared_error(y_test, y_hat_model9)\n",
        "rmse9 = np.sqrt(mse9)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse9}')"
      ],
      "metadata": {
        "id": "wgkUdcc2MmIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 10: Modelo 4 con interacciones y no linealidades de tercer grado"
      ],
      "metadata": {
        "id": "MQGTfWwbM-Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "X_train_poly_3 = poly_3.fit_transform(X_train[[\"edad\", \"informal\", \"sexo\"]+columns_to_use_4])\n",
        "model10 =  LinearRegression().fit(X_train_poly_3,y_train)"
      ],
      "metadata": {
        "id": "4sbDTuDbNFCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 10\n",
        "X_test_poly_10 = poly_3.fit_transform(X_test[[\"edad\", \"informal\", \"sexo\"]+columns_to_use_4])\n",
        "y_hat_model10 = model10.predict(X_test_poly_10)\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse10 = mean_squared_error(y_test, y_hat_model10)\n",
        "rmse10 = np.sqrt(mse10)\n",
        "\n",
        "print(f'Mean Squared Error: {rmse10}')"
      ],
      "metadata": {
        "id": "iEfrnrhXNKWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 11: Modelo 5 con interacciones y no linealidades de segundo grado\n",
        "\n"
      ],
      "metadata": {
        "id": "HU5Tpfk8NtuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "X_train_poly_2 = poly_2.fit_transform(X_train[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_5])\n",
        "model11 =  LinearRegression().fit(X_train_poly_2,y_train)"
      ],
      "metadata": {
        "id": "otE6vH9ANyKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 11\n",
        "X_test_poly_11 = poly_2.fit_transform(X_test[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_5])\n",
        "y_hat_model11 = model11.predict(X_test_poly_11)\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse11 = mean_squared_error(y_test, y_hat_model11)\n",
        "rmse11 = np.sqrt(mse11)\n",
        "\n",
        "print(f'Mean Squared Error: {rmse11}')"
      ],
      "metadata": {
        "id": "iojmIu-eOxdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 12: Modelo 5 con interacciones y no linealidades de tercer grado\n"
      ],
      "metadata": {
        "id": "Ncai8g4aPVU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "X_train_poly_3 = poly_3.fit_transform(X_train[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_5])\n",
        "model12 =  LinearRegression().fit(X_train_poly_3,y_train)"
      ],
      "metadata": {
        "id": "ef0IzYJXPYri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 12\n",
        "X_test_poly_12 = poly_3.fit_transform(X_test[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_5])\n",
        "y_hat_model12 = model12.predict(X_test_poly_12)\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse12 = mean_squared_error(y_test, y_hat_model12)\n",
        "rmse12 = np.sqrt(mse12)\n",
        "\n",
        "print(f'Mean Squared Error: {rmse12}')"
      ],
      "metadata": {
        "id": "mN5ptMaJPb_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 13: Modelo 6 con interacciones y no linealidades de segundo grado\n",
        "\n"
      ],
      "metadata": {
        "id": "hm8A2t0KP1vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "X_train_poly_2 = poly_2.fit_transform(X_train[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_6])\n",
        "model13 =  LinearRegression().fit(X_train_poly_2,y_train)"
      ],
      "metadata": {
        "id": "KFiM6X4RP230"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 13\n",
        "X_test_poly_13 = poly_2.fit_transform(X_test[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_6])\n",
        "y_hat_model13 = model13.predict(X_test_poly_13)\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse13 = mean_squared_error(y_test, y_hat_model13)\n",
        "rmse13 = np.sqrt(mse13)\n",
        "\n",
        "print(f'Mean Squared Error: {rmse13}')"
      ],
      "metadata": {
        "id": "D7fdHvDhP9o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 14: Modelo 6 con interacciones y no linealidades de tercer grado\n"
      ],
      "metadata": {
        "id": "pISVlqHIQHLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino el modelo\n",
        "X_train_poly_3 = poly_3.fit_transform(X_train[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_6])\n",
        "model14 =  LinearRegression().fit(X_train_poly_3,y_train)"
      ],
      "metadata": {
        "id": "pI9tFu69QMij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones basadas en el modelo 14\n",
        "X_test_poly_14 = poly_3.fit_transform(X_test[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_6])\n",
        "y_hat_model14 = model14.predict(X_test_poly_14)\n",
        "\n",
        "# Calculamos el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE)\n",
        "mse14 = mean_squared_error(y_test, y_hat_model14)\n",
        "rmse14 = np.sqrt(mse14)\n",
        "\n",
        "print(f'Mean Squared Error: {rmse14}')"
      ],
      "metadata": {
        "id": "h3SgdQtOQPyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resumen en DF"
      ],
      "metadata": {
        "id": "Rc1mWsQcQo1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de nombres de los modelos\n",
        "model_names = [\n",
        "    \"Modelo 1\", \"Modelo 2\", \"Modelo 3\", \"Modelo 4\",\n",
        "    \"Modelo 5\", \"Modelo 6\", \"Modelo 7\", \"Modelo 8\",\n",
        "    \"Modelo 9\", \"Modelo 10\", \"Modelo 11\", \"Modelo 12\",\n",
        "    \"Modelo 13\", \"Modelo 14\"\n",
        "]\n",
        "\n",
        "# Lista de los MSE calculados\n",
        "mse_values = [rmse1, rmse2, rmse3, rmse4, rmse5, rmse6, rmse7, rmse8, rmse9, rmse10, rmse11, rmse12, rmse13, rmse14]\n",
        "\n",
        "# Crear un DataFrame con los resultados\n",
        "mse_df = pd.DataFrame({\n",
        "    \"Modelo\": model_names,\n",
        "    \"MSE\": mse_values\n",
        "})\n",
        "\n",
        "# Mostrar la tabla\n",
        "print(mse_df)"
      ],
      "metadata": {
        "id": "sWMLzdBNQoCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Errores de los modelos 1 y 14"
      ],
      "metadata": {
        "id": "KHyOjezBkpkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino modelos\n",
        "models = [\n",
        "    (\"Modelo 1\", y_hat_model1),\n",
        "    (\"Modelo 14\", y_hat_model14)\n",
        "]\n",
        "\n",
        "# Grafico distribución de los errores\n",
        "for name, y_hat in models:\n",
        "    # Calcular los errores\n",
        "    residuals = y_test - y_hat.flatten()\n",
        "\n",
        "    # Graficar la distribución de los errores\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(residuals, kde=True, bins=30, alpha=0.7)\n",
        "    plt.title(f\"Distribución de los Errores - {name}\", fontsize=16)\n",
        "    plt.xlabel(\"Errores\", fontsize=14)\n",
        "    plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Guardo el plot como PNG file\n",
        "    plt.savefig(f\"{name}_residuals_distribution.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "# Modelo 1\n",
        "\n",
        "# Graficar valores reales vs predicciones\n",
        "plt.figure(figsize=(8, 6))  # Tamaño de la figura\n",
        "plt.scatter(range(len(y_test)), y_test, label='Valores reales', alpha=0.6)\n",
        "plt.plot(range(len(y_test)), y_hat_model1, color='red', label='Predicciones', alpha=0.7, linestyle='-', linewidth=2)\n",
        "plt.title(\"Valores Reales vs Predicciones - Modelo 1\", fontsize=16)\n",
        "plt.xlabel(\"Observación\", fontsize=14)\n",
        "plt.ylabel(\"Salario Horario\", fontsize=14)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()  # Ajusta el diseño\n",
        "plt.show()\n",
        "\n",
        "# Guardo el plot como PNG file\n",
        "plt.savefig(\"modelo1_residuals_scatterplot.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Modelo 14\n",
        "\n",
        "# Graficar valores reales vs predicciones\n",
        "plt.figure(figsize=(8, 6))  # Tamaño de la figura\n",
        "plt.scatter(range(len(y_test)), y_test, label='Valores reales', alpha=0.6)\n",
        "plt.scatter(range(len(y_test)), y_hat_model14, color='red', label='Predicciones', alpha=0.7, linestyle='-', linewidth=2)\n",
        "plt.title(\"Valores Reales vs Predicciones - Modelo 14\", fontsize=16)\n",
        "plt.xlabel(\"Observación\", fontsize=14)\n",
        "plt.ylabel(\"Salario Horario\", fontsize=14)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()  # Ajusta el diseño\n",
        "plt.show()\n",
        "\n",
        "# Guardo el plot como PNG file\n",
        "plt.savefig(f\"./modelo14_residuals_scatterplot.png\", dpi=300)\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "ZRiuj86io9tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el Leave-One-Out Cross-Validation (LOOCV)\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Para Modelo 1 (solo el intercepto)\n",
        "mse1_list = []\n",
        "\n",
        "for train_index, test_index in loo.split(X_train):\n",
        "    X_train_loo, X_test_loo = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "    y_train_loo, y_test_loo = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    model1 = LinearRegression().fit(np.ones((len(X_train_loo), 1)), y_train_loo)\n",
        "\n",
        "    # Predecir\n",
        "    y_hat_model1 = model1.predict(np.ones((len(X_test_loo), 1)))\n",
        "\n",
        "    # Calcular el MSE\n",
        "    mse1_list.append(mean_squared_error(y_test_loo, y_hat_model1))\n",
        "\n",
        "# Promedio de MSE para el modelo 1\n",
        "mse1_loo = np.mean(mse1_list)\n",
        "print(f'Mean Squared Error para Modelo 1 (LOOCV): {mse1_loo}')\n",
        "\n",
        "# Para Modelo 14\n",
        "mse14_list = []\n",
        "\n",
        "for train_index, test_index in loo.split(X_train[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_6]):\n",
        "    X_train_loo, X_test_loo = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "    y_train_loo, y_test_loo = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    model14 = LinearRegression().fit(X_train_loo[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_6], y_train_loo)\n",
        "\n",
        "    # Predecir\n",
        "    y_hat_model14 = model14.predict(X_test_loo[[\"edad\", \"informal\", \"antig\", \"sexo\"]+columns_to_use_6])\n",
        "\n",
        "    # Calcular el MSE\n",
        "    mse14_list.append(mean_squared_error(y_test_loo, y_hat_model14))\n",
        "\n",
        "# Promedio de MSE para el modelo 14\n",
        "mse14_loo = np.mean(mse14_list)\n",
        "print(f'Mean Squared Error para Modelo 14 (LOOCV): {mse14_loo}')\n"
      ],
      "metadata": {
        "id": "NPU79-rldgzZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}